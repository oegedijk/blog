{
  
    
        "post0": {
            "title": "Serializing with dependencies in python",
            "content": "Summary: . If you wish to store (pickle) python objects into a single pickle file without accompanying modules with class/function definitions, you should: . Use dill instead of pickle | Use some tricks to trick dill into believing that imports have actually been defined in __main__ | Pickling with dependencies . When you want to pickle a python object for long term storage, you can run into a problem: pickle does not store object definitions when it pickles. So for example when you build a class Greeter and then pickle it and unpickle it at another location, you already need to have class Greeter correctly defined before you can load the pickle at the target destination. . def greeting1(): return &quot;Booyaa!&quot; def greeting2(): return &quot;Howdy!&quot; class Greeter: def __init__(self, greetings): self.greetings = greetings def greet(self): for greet in self.greetings: print(greet()) . greeter = Greeter([greeting1, greeting2]) greeter.greet() . Booyaa! Howdy! . import pickle pickle.dump(greeter, open(&quot;greeter.pkl&quot;, &quot;wb&quot;)) . If you now try to load the greeter somewhere else, you will get an AttributeError: . &gt; &gt;&gt; import pickle &gt;&gt;&gt; pickle.load(open(&quot;greeter.pkl&quot;, &quot;rb&quot;)) Traceback (most recent call last):File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: Can&#39;t get attribute &#39;Greeter&#39; on &lt;module &#39;__main__&#39; (built-in)&gt; . Solution 1: Also store the module with definitions . One way around this by storing the class definition of Greeter in greetings.py, import Greeter from greetings, and copy greetings.py along with greeter.pkl to the location where you want to unpickle. . However, now you have created a dependency that you need to manage. You always have to make sure that you have the right version of the right module at hand when you want to unpickle. Especially for long term storage of python objects, this is begging for problems. It would be nicer if you could have the object itself and the definition all in one file! . Solution 2: Dill to the rescue! . Luckily there is a stand-in replacement for pickle called dill that unfortunately does not come with the standard library, so you have to install it yourself: pip install dill. . The nice thing about dill is that it stores definitions along with the object, as long as they are defined in __main__. In our case they are so when we store the greeter instance with dill we can actually reload the object now: . import dill dill.dump(greeter, open(&quot;greeter.pkl&quot;, &quot;wb&quot;)) . &gt;&gt;&gt; import dill &gt;&gt;&gt; greeter = dill.load(open(&quot;greeter.pkl&quot;, &quot;rb&quot;)) &gt;&gt;&gt; greeter.greet() Booyaa! Howdy! . It worked! . But only stores definitions in __main__ not in modules . Suppose we define Greeter in a greeter.py module: . greeter.py: . def greeting1(): return &quot;Booyaa!&quot; def greeting2(): return &quot;Howdy!&quot; class Greeter: def __init__(self, greetings): self.greetings = greetings def greet(self): for greet in self.greetings: print(greet()) . import dill from greeter import Greeter, greeting1, greeting2 g = Greeter([greeting1, greeting2]) dill.dump(g, open(&quot;greeter.pkl&quot;, &quot;wb&quot;)) . Then the pickle again fails to load when greetings.py is either missing or misses the right definitions: . &gt; &gt;&gt; import dill &gt;&gt;&gt; greeter = dill.load(open(&quot;_notebooks/greeter.pkl&quot;, &quot;rb&quot;)) Traceback (most recent call last):File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/Users/oege/.pyenv/versions/3.8.5/lib/python3.8/site-packages/dill/_dill.py&quot;, line 278, in load return Unpickler(file, ignore=ignore, **kwds).load() File &quot;/Users/oege/.pyenv/versions/3.8.5/lib/python3.8/site-packages/dill/_dill.py&quot;, line 481, in load obj = StockUnpickler.load(self) File &quot;/Users/oege/.pyenv/versions/3.8.5/lib/python3.8/site-packages/dill/_dill.py&quot;, line 471, in find_class return StockUnpickler.find_class(self, module, name) ModuleNotFoundError: No module named &#39;greeter&#39; . Workaround: move definitions to __main__: . You can work around this problem by mainifying the imported definitions: . import dill from greeter import Greeter, greeting1, greeting2 def mainify(obj): &quot;&quot;&quot;If obj is not defined in __main__ then redefine it in main so that dill will serialize the definition along with the object&quot;&quot;&quot; if obj.__module__ != &quot;__main__&quot;: import __main__ import inspect s = inspect.getsource(obj) co = compile(s, &#39;&lt;string&gt;&#39;, &#39;exec&#39;) exec(co, __main__.__dict__) mainify(Greeter) mainify(greeting1) mainify(greeting2) print(Greeter.__module__, greeting1.__module__, greeting2.__module__) g = Greeter([greeting1, greeting2]) dill.dump(g, open(&quot;greeter.pkl&quot;, &quot;wb&quot;)) . __main__ __main__ __main__ . And this works: . &gt;&gt;&gt; import dill &gt;&gt;&gt; greeter = dill.load(open(&quot;_notebooks/greeter.pkl&quot;, &quot;rb&quot;)) &gt;&gt;&gt; greeter.greet() Booyaa! Howdy! . Avoid using mainify in __main__ . It is a bit cumbersome to mainify everything though, and you may wish to automate this for your users. . One way of doing this is by declaring a classmethod dillable: . using @classmethod . greeter2.py: . class Greeting: def __init__(self, greetings): self.greetings = greetings @classmethod def dillable(cls, greetings): import __main__ for greeting in greetings: cls._mainify(greeting) cls._mainify(cls) cls = getattr(__main__, cls.__name__) greetings = [getattr(__main__, greeting.__name__) for greeting in greetings] return cls(greetings) @staticmethod def _mainify(obj): &quot;&quot;&quot;If obj is not defined in __main__ then redefine it in main so that dill will serialize the definition along with the object&quot;&quot;&quot; if obj.__module__ != &quot;__main__&quot;: import __main__ import inspect s = inspect.getsource(obj) co = compile(s, &#39;&lt;string&gt;&#39;, &#39;exec&#39;) exec(co, __main__.__dict__) . Now you can import in main: . import dill from greeter2 import Greeter, greeting1, greeting2 g = Greeter.dillable([greeting1, greeting2]) g.greet() dill.dump(g, open(&quot;greeter.pkl&quot;, &quot;wb&quot;)) . Booyaa! Howdy! . using __new__: . greeter3.py: . class Greeting: def __init__(self, greetings): self.greetings = greetings def __new__(cls, greetings=None): import __main__ if greetings is not None: cls._mainify(cls) cls = getattr(__main__, cls.__name__) obj = object.__new__(cls) if greetings is not None: for greeting in greetings: cls._mainify(greeting) greetings = [getattr(__main__, greeting.__name__) for greeting in greetings] obj.__init__(greetings) return obj @staticmethod def _mainify(obj): &quot;&quot;&quot;If obj is not defined in __main__ then redefine it in main so that dill will serialize the definition along with the object&quot;&quot;&quot; if obj.__module__ != &quot;__main__&quot;: import __main__ import inspect s = inspect.getsource(obj) co = compile(s, &#39;&lt;string&gt;&#39;, &#39;exec&#39;) exec(co, __main__.__dict__) . import dill from greeter3 import Greeter, greeting1, greeting2 g = Greeter([greeting1, greeting2]) g.greet() dill.dump(g, open(&quot;greeter.pkl&quot;, &quot;wb&quot;)) . Booyaa! Howdy! . Conclusion: . Hope this may be useful for some of you that want to store python objects and not worry about module dependencies! .",
            "url": "https://oegedijk.github.io/blog/pickle/dill/python/2020/11/10/serializing-dill-references.html",
            "relUrl": "/pickle/dill/python/2020/11/10/serializing-dill-references.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Callable properties in python",
            "content": "Why callable properties? . First of all: you probably should not use callable properties if you can somehow avoid it :) . The two main reasons to use callable properties in python that I see are the following: . Playing around with the dynamic nature of python for the fun of it | You had defined @properties in your API that later on you realized should have been a get_ method with a parameter, but you do not want break the old API. | (I had a case of no. 2) . Adding callable indexes to properties . Suppose we had built ourselves a nice little class that holds a list of fruit. At any given time only one fruit is the selected fruit, indicated by the index self.idx. . We use a @property to return the current selected fruit from the list: . class FruitList: def __init__(self, fruits, idx=0): self.fruits = fruits self.idx = idx @property def selected_fruit(self): return self.fruits[self.idx] . fl = FruitList([&#39;apple&#39;, &#39;banana&#39;, &#39;pear&#39;], idx=1) print(&quot;init idx == 1, fruit = &quot;, fl.selected_fruit) fl.idx = 2 print(&quot;new idx == 2, fruit = &quot;, fl.selected_fruit) . init idx == 1, fruit = banana new idx == 2, fruit = pear . Your users have been happily using your FruitList class and seem happy with the API, but now a new request comes in: would it be possible to explicitly select a particular fruit by index? . You could ofcourse add a method . def get_selected_fruit(self, index=None): if index is not None: return self.fruits[self.index] else return self.fruits[self.idx] . But now we have duplicate way of getting selected fruit, which makes the API more confusing. You could deprecate the old property, but that might break the API for older users. . So what if there would be a way to keep the old API functional, plus add the new functionality by doing one weird python trick? . Enter callable default lists . We can do this by creating an object that: . Is equal to a default index element when not called | Returns the default index oelement when called without parameters | Returns a specific index element when called with an index parameter | If the @property would return such an object, then both the old API and new index selector API could be supported by a single @property! . The trick is creating a new class that is an instance of the default index item (e.g. for &#39;banana&#39; the type would be str), but also contains the full list of fruit, and has a __call__ method that returns the right fruit when called with a specific index, and otherwise just returns the default fruit. . Or in Python code: . from typing import List def make_callable_default_list(source_list:List, default_index:int): class DefaultList(type(source_list[default_index])): def __new__(cls, default_value, source_list): obj = type(source_list[default_index]).__new__(cls, default_value) return obj def __init__(self, default_value, source_list): super().__init__() self.source_list = source_list self.default_type = type(default_value) def __call__(self, index=None): if index is not None: return self.source_list[index] else: return self.default_type(self) return DefaultList(source_list[default_index], source_list) . Now we simply make the property return such a DefaultList: . class FruitList: def __init__(self, fruits, idx=0): self.fruits = fruits self.idx = idx @property def selected_fruit(self): return make_callable_default_list(self.fruits, self.idx) . When calling selected_fruit as an attribute, it still works the same as before for our old users: . fl = FruitList([&#39;apple&#39;, &#39;banana&#39;, &#39;pear&#39;], idx=1) print(&quot;Init self.idx == 1, fruit = &quot;, fl.selected_fruit) fl.idx = 2 print(&quot;Set self.idx = 2, fruit = &quot;, fl.selected_fruit) . Init self.idx == 1, fruit = banana Set self.idx = 2, fruit = pear . But now the property also works as a callable to get the fruit of a specific index for our new users: . print(&quot;Using callable without index, fruit = &quot;, fl.selected_fruit()) print(&quot;Specifying idx = 0, fruit = &quot;, fl.selected_fruit(0)) . Using callable without index, fruit = pear Specifying idx = 0, fruit = apple . Using pd.DataFrame, pd.Series or np.ndarray as list elements . The above should work with most typical python objects, but if you happen to want to return a pd.DataFrame or pd.Series or a np.ndarray, you need to slightly alter the code to get it to work, as these types are special in the way they are initialized. Below however some code that should work for all these types: . class DefaultDfList(pd.DataFrame): &quot;&quot;&quot;&quot; You have the set source_list manually! e.g. dfl = DefaultDfList(df1) dfl.source_list = [df1, df2] &quot;&quot;&quot; _internal_names = list(pd.DataFrame._internal_names) + [&#39;source_list&#39;] _internal_names_set = set(_internal_names) def __call__(self, index=None): if index is not None: return self.source_list[index] else: return pd.DataFrame(self) @property def _constructor(self): return DefaultDfList class DefaultSeriesList(pd.Series): _internal_names = list(pd.Series._internal_names) + [&#39;source_list&#39;] _internal_names_set = set(_internal_names) def __call__(self, index=None): if index is not None: return self.source_list[index] else: return pd.Series(self) @property def _constructor(self): return DefaultSeriesList class DefaultNpArrayList(np.ndarray): def __new__(cls, default_array, source_list): obj = np.asarray(default_array).view(cls) obj.source_list = source_list return obj def __array_finalize__(self, obj): if obj is None: return self.source_list = getattr(obj, &#39;source_list&#39;, None) def __array_wrap__(self, out_arr, context=None): return np.ndarray.__array_wrap__(self, out_arr, context).view(np.ndarray) def __call__(self, index=None): if index is not None: return self.source_list[index] return self.view(np.ndarray) def default_list(source_list:List, default_index:int): &quot;&quot;&quot; Normally gives the default_index item in a list. If used as a callable, you can specify a specific index. Use to make @property that you can pass optional index parameter to &quot;&quot;&quot; if isinstance(source_list[default_index], pd.DataFrame): df_list = DefaultDfList(source_list[default_index]) df_list.source_list = source_list return df_list if isinstance(source_list[default_index], pd.Series): s_list = DefaultSeriesList(source_list[default_index]) s_list.source_list = source_list return s_list if isinstance(source_list[default_index], np.ndarray): a_list = DefaultNpArrayList(source_list[default_index], source_list) return a_list class DefaultList(type(source_list[default_index])): def __new__(cls, default_value, source_list): obj = type(source_list[default_index]).__new__(cls, default_value) return obj def __init__(self, default_value, source_list): super().__init__() self.source_list = source_list self.default_type = type(default_value) def __call__(self, index=None): if index is not None: return self.source_list[index] else: return self.default_type(self) return DefaultList(source_list[default_index], source_list) . Conclusion . So there you have a nice example of how the dynamic nature of python allows you to do some pretty crazy things with your API. . Whether this is a good idea is ofcourse another question :) .",
            "url": "https://oegedijk.github.io/blog/python/2020/11/10/callable-properties.html",
            "relUrl": "/python/2020/11/10/callable-properties.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introducing explainerdashboard",
            "content": "As data scientists working in a public or regulated sector we are under increasing pressure to make sure that our machine learning models are transparent, explainable and fair. With the advent of tools such as SHAP and LIME, the old black-box trope actually does not really apply anymore, and it has become quite straightforward to explain how each feature contributed to each individual prediction for example. However straightforward for a data scientist is not the same as straightforward for a manager, supervisor or regulator. And so what is needed is a tool that allows non-technical stakeholders to inspect the workings, performance and predictions of a machine learning model without needing to learn Python or getting the hang of Jupyter notebooks.  . With the explainerdashboard python package, building, deploying and sharing interactive dashboards that allow non-technical users to explore the inner workings of a machine learning model can be done with just two lines of code. For example to build this example hosted at titanicexplainer.herokuapp.com/classifier, you just to need to fit a model: . from sklearn.ensemble import RandomForestClassifier from explainerdashboard.datasets import titanic_survive X_train, y_train, X_test, y_test = titanic_survive() model = RandomForestClassifier().fit(X_train, y_train) . And pass it to an Explainer object: . from explainerdashboard import ClassifierExplainer explainer = ClassifierExplainer(model, X_test, y_test) . And then you simply pass this explainer to an ExplainerDashboard and run it: . from explainerdashboard import ExplainerDashboard ExplainerDashboard(explainer).run() . This will launch a dashboard built on top off plotly dash that will run on http://localhost:8050 by default. . With this dashboard you can for example see which features are the most important to the model: . . Or how the model performs: . . And you can explain how each individual feature contributed to each individual prediction: . . Figure out how predictions would have changed if one or more of the variables were different: . . See how feature impact predictions : . . And even inspect every decision tree inside a random forest: . . –&gt; .",
            "url": "https://oegedijk.github.io/blog/markdown/2020/10/15/introducing-explainerdashboard.html",
            "relUrl": "/markdown/2020/10/15/introducing-explainerdashboard.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Putting SHAP into production with Amazon Lambda.",
            "content": "Deploying explainable models to AWS Lambda using Zappa . Nowadays fairness and transparancy are becoming more and more important for machine learning applications. Especially when your machine learning model will be used to make decisions with a large effect on people’s wellbeing such as acceptance or rejections of loan applications, fraud investigations, etc. . Building some unscrutable black-box deep learning algorithms and claiming it has an accuracy of over 90% is no longer good enough. Models and model predictions should be explainable to both regulators and end users. . However with modern approaches such as SHAP values (Lundberg et al 2017; 2018), the contributions of each feature to the final predictions can be calculated, thus providing an explanation for how the model used the inputs to reach its final prediction. The challenge is however to integrate such SHAP values into a production system, in order to provide transparent model decisions to stakeholders, decision-makers, regulators and customers alike. . Here we will be focusing on getting our model into production using Amazon Lambda functions and the awesome zappa package. The advantage of lambda functions is that they are serverless: they don’t need to be hosted on always-on infrastructure like for example sagemaker endpoints (or your own EC2 deployment or on premise solution). Only when they get called does amazon find a server to run the specific code on. The downside is that there are a lot more restructions on the size of the code and dependencies and infrastructure. (for example no GPUs sadly for deep learning deployments). All dependencies also have to be compatible with a particular flavor of Amazon Linux. . Deploying and properly configuring an AWS lambda function and Gateway API (to get a public face URL) can be quite tricky and time consuming. Luckily there is an amazing almost magical package that simplifies all of this for python projects called (Zappa)[https://github.com/Miserlou/Zappa]. . Zappa takes care of all the configuration behind the scenes: bundling up all the code and dependencies, setting up S3 buckets to store artifacts, setting up the lambda function, configuring the Gateway API, setting up logging, and just giving you nice API url in the end. . For standard scikit-learn models zappa provides lambda compatible versions of numpy, pandas and scikit-learn. However in order to deploy a model that also returns shap values, we have to build our environment inside a docker container as you will see. . Zappa basics . The basics steps of deploying with zappa are: . Create a virtual environment | Install zappa into this venv along with other dependencies | Call zappa init or provide your own zappa_settings.json | call zappa deploy. | Zappa automatically bundles up all packages in your venv in a zip file and sends it to lambda to be deployed. If the package is too large for lambda limits (50MB zipped), you can set &quot;slim_handler&quot;: true, and zappa will only deploy a loader package to lambda, store the actual package in an S3 bucket, and then download the package to lambda as needed in order to bypass the limit. . Zappa tutorial . I highly recommend reading through this tutorial to find out what zappa does, how to configure your permissions and deploy your first toy API to AWS lambda: (https://pythonforundergradengineers.com/deploy-serverless-web-app-aws-lambda-zappa.html) [https://pythonforundergradengineers.com/deploy-serverless-web-app-aws-lambda-zappa.html] . Especially setting up the right permissions can be quite challenging. After following this tutorial you should have a user set up with the right credentials and saved the credentials to ~/.aws/credentials. . Deploying a ML API including shap . When deploying to lambda you need to make sure that all packages are compatible with the Lambda AWS Linux environment. For standard packages such as numpy, pandas and scikit-learn, zappa already has pre-built packages prepared and will automatically substitute these for you. (told you it was magic!) . However shap is not one of those so you have to build compatible versions yourself. The easiest way to do that is to install them inside a lambda compatible docker container. . You can start from the lambci/lambda:build-python3.7 image, and then run it in interactive mode. To have acces to your credentials you can mount the ~/.aws directory to /root/.aws: . docker run --rm -it -v $(pwd):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash . Then you create the venv: python -m venv lambda_env, activate it: source lambda_env/bin/activate, and install the dependencies pip install -r requirements.txt. . The next step is to create a zappa_settings.json file by calling zappa init. The default options are fine. You then open the newly created zappa_settings.json and add the following lines: . &quot;slim_handler&quot;: true, // for large packages, store package in S3 &quot;aws_region&quot;: &quot;eu-west-1&quot;, // or whatever your aws region is &quot;keep_warm&quot;: false, // if you want to disable to default keep warm callback . If instead of calling zappa init you use the zappa_settings.json file in this repo, then also make sure to change the &quot;s3_bucket&quot; to something uniquely yours. . Finally you call zappa deploy dev (assuming you named the task dev), and your api should get deployed. Like magic! . Note the url provided by GatewayAPI at the end, and go test your API! . Using Makefile . In this project is also an example Makefile so that you can simply call make env model deployto build and deploy the model, and then make undeploy clean to undeploy the project and clean up. . Conclusion . Setting up lambda functins using zappa can be a breeze as long as you build your environment in a proper lambda compatible environment. . Appendix . Makefile . Below the contents of the Makefile: . all: help help: ## Show this help. @fgrep -h &quot;##&quot; $(MAKEFILE_LIST) | fgrep -v fgrep | sed -e &#39;s/ $$//&#39; | sed -e &#39;s/##//&#39; env: ## build lambci compatible venv and install requirements.txt docker run --rm -v $(CURDIR):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash -c &quot; python -m venv lambda_env &amp;&amp; source lambda_env/bin/activate &amp;&amp; pip install -r requirements.txt &amp;&amp; zappa init&quot; model: ## build the model artifacts: model.pkl, imputer.pkl, explainer.pkl docker run --rm -v $(CURDIR):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash -c &quot; source lambda_env/bin/activate &amp;&amp; python build_model_artifacts.py&quot; deploy: ## activate venv and deploy docker run --rm -v $(CURDIR):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash -c &quot; source lambda_env/bin/activate &amp;&amp; zappa deploy dev&quot; update: ## activate venv and update deployment docker run --rm -v $(CURDIR):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash -c &quot; source lambda_env/bin/activate &amp;&amp; zappa update dev --yes&quot; undeploy: ## activate venv and undeploy docker run --rm -v $(CURDIR):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash -c &quot; source lambda_env/bin/activate &amp;&amp; zappa undeploy dev --yes&quot; interactive: docker run --rm -it -v $(CURDIR):/var/task -v ~/.aws:/root/.aws lambci/lambda:build-python3.7 /bin/bash clean: rm -r lambda_env rm pkl/* .",
            "url": "https://oegedijk.github.io/blog/markdown/2020/08/15/lambda-explainer.html",
            "relUrl": "/markdown/2020/08/15/lambda-explainer.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Putting SHAP into production with Amazon Sagemaker.",
            "content": "Deploying explainable models to AWS Sagemaker . Nowadays fairness and transparancy are becoming more and more important for machine learning applications. Especially when your machine learning model will be used to make decisions with a large effect on people’s wellbeing such as acceptance or rejections of loan applications, fraud investigations, etc. . Building some unscrutable black-box deep learning algorithms and claiming it has an accuracy of over 90% is no longer good enough. Models and model predictions should be explainable to both regulators and end users. . However with modern approaches such as SHAP values (Lundberg et al 2017; 2018), the contributions of each feature to the final predictions can be calculated, thus providing an explanation for how the model used the inputs to reach its final prediction. The challenge is however to integrate such SHAP values into a production system, in order to provide transparent model decisions to stakeholders, decision-makers, regulators and customers alike. . Here we will be focusing on getting our model into production using Amazon Sagemaker. . It turns out that the shap library is not included by default in sagemaker estimator docker images, so it will not work out of the box. Which means that in order to provide shap values as part of your output, you have to build a custom docker container. Which is doable but a little but more complicated than usual. . The other part that is different from standard models is constructing the response payload to include shap values. But that should be the easy part. . Github Repository . All the code and examples mentioned in this blog post are hosted at https://github.com/oegedijk/sagemaker-creditscore-explainer . notebook . It is easiest to deploy sagemaker containers, models and endpoints from within a sagemaker notebook instance. So attach the git repository https://github.com/oegedijk/sagemaker-creditscore-explainer to your sagemaker notebook instance. . More info on how to attach a git repo to your notebook instance here: https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-git-repo.html . After you have connected to the repo, go to the sagemaker/notebooks directory and open sagemaker_explainer.ipynb. . ECR Container . Given that we will be deploying a custom docker container, we need to make sure we have a ECR docker registry set up. . You can go the the AWS console, find Elastic Container Registry and create one in your AWS region. For example, to create one in eu-central-1 go to https://eu-central-1.console.aws.amazon.com/ecr/repositories?region=eu-central-1) . The name I gave to my repository is sagemaker-explainer. . setting ECR permissions . In order to create a custom docker container for our model we need to add AmazonEC2ContainerRegistryFullAccess policy to our notebook. . In the sagemaker console: . click on notebook instances | click on the notebook instance that you are using | go to Permissions and encryption | click on the IAM role ARN | click on ‘Attach Policies’ | find AmazonEC2ContainerRegistryFullAccess | add it to the notebook. | . Attach additional policies: . You may have to add some additional permissions to your notebook policies, namely &quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:BatchGetImage&quot; and &quot;ecr:BatchCheckLayerAvailability&quot;. . You can either edit these manually, or paste the following json: . { &quot;Version&quot;: &quot;2008-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;allowSageMakerToPull&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:BatchGetImage&quot;, &quot;ecr:BatchCheckLayerAvailability&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } . Installing docker-credential-ecr-login . In order to log into ECR from our sagemaker notebook, we need to install a tool called docker-credential-ecr-login. We download and install it inside out Sagemaker notebook with: . !sudo wget -P /usr/bin https://amazon-ecr-credential-helper-releases.s3.us-east-2.amazonaws.com/0.4.0/linux-amd64/docker-credential-ecr-login . and . !sudo chmod +x /usr/bin/docker-credential-ecr-login . Dockerfile . The Dockerfile to create our custom container is quite straightforward and can be found in sagemaker/container/Dockerfile: . ARG SCIKIT_LEARN_IMAGE FROM $SCIKIT_LEARN_IMAGE COPY requirements.txt /requirements.txt RUN pip install --no-cache -r /requirements.txt &amp;&amp; rm /requirements.txt . So basically we take a scikit learn image (defined by a parameter) and then install additional requirements into it (basically joblib and shap). . DockerImage deployment helper class . In order to build and push our custom image we make use of this nice helper class: . ecr_client = boto3.client(&quot;ecr&quot;, region_name=AWS_REGION) docker_client = docker.APIClient() class DockerImage: def __init__(self, registry, repository_name, tag=&quot;latest&quot;, docker_config_filepath=&#39;/home/ec2-user/.docker/config.json&#39;): self.registry = registry self.repository_name = repository_name self.docker_config_filepath = docker_config_filepath self.tag = tag self._check_credential_manager() self._configure_credentials() def __str__(self): return &quot;{}/{}:{}&quot;.format(self.registry, self.repository_name, self.tag) @property def repository(self): return &quot;{}/{}&quot;.format(self.registry, self.repository_name) @property def short_name(self): return self.repository_name @staticmethod def _check_credential_manager(): try: subprocess.run( [&quot;docker-credential-ecr-login&quot;, &quot;version&quot;], stdout=subprocess.DEVNULL, ) except Exception: raise Exception( &quot;Couldn&#39;t run &#39;docker-credential-ecr-login&#39;. &quot; &quot;Make sure it is installed and configured correctly.&quot; ) def _configure_credentials(self): docker_config_filepath = Path(self.docker_config_filepath) if docker_config_filepath.exists(): with open(docker_config_filepath, &quot;r&quot;) as openfile: docker_config = json.load(openfile) else: docker_config = {} if &quot;credHelpers&quot; not in docker_config: docker_config[&quot;credHelpers&quot;] = {} docker_config[&quot;credHelpers&quot;][self.registry] = &quot;ecr-login&quot; docker_config_filepath.parent.mkdir(exist_ok=True, parents=True) with open(docker_config_filepath, &quot;w&quot;) as openfile: json.dump(docker_config, openfile, indent=4) def build(self, dockerfile, buildargs): path = Path(dockerfile).parent for line in docker_client.build( path=str(path), buildargs=buildargs, tag=self.repository_name, decode=True, ): if &quot;error&quot; in line: raise Exception(line[&quot;error&quot;]) else: print(line) def push(self): docker_client.tag( self.repository_name, self.repository, self.tag, force=True ) for line in docker_client.push( self.repository, self.tag, stream=True, decode=True ): print(line) . Getting scikit-learn image URI . So now we can get our scikit-learn image: . def scikit_learn_image(): registry = sagemaker.fw_registry.registry( region_name=AWS_REGION, framework=&quot;scikit-learn&quot; ) repository_name = &quot;sagemaker-scikit-learn&quot; tag = &quot;0.20.0-cpu-py3&quot; return DockerImage(registry, repository_name, tag) sklearn_image = scikit_learn_image() . Building custom image based on scikit-learn image . And use that to build and push our custom image: . def custom_image(aws_account_id, aws_region, repository_name, tag=&quot;latest&quot;): ecr_registry = f&quot;{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com&quot; return DockerImage(ecr_registry, repository_name, tag) custom_image = custom_image(AWS_ACCOUNT_ID, AWS_REGION, ECR_REPOSITORY_NAME) dockerfile = Path.cwd().parent / &quot;container&quot; / &quot;Dockerfile&quot; custom_image.build( dockerfile=dockerfile, buildargs={&#39;SCIKIT_LEARN_IMAGE&#39;: str(sklearn_image)} ) custom_image.push() . This will take some time, but by the end you should have a custom training image with shap installed. The URI will be along the lines of . ‘AWS_ACCOUNT_ID###.dkr.ecr.AWS_REGION###.amazonaws.com/sagemaker-explainer:latest’ . Training the Model . Sagemaker Estimator . Now that we have our custom training image, we can make use of the builtin Sagemaker SKLearn estimator, as long as we make sure to point it towards our custom image: . estimator = SKLearn( image_name=str(custom_image), entry_point=&#39;entry_point.py&#39;, source_dir=str(source_dir), hyperparameters=hyperparameters, role=role, train_instance_count=1, train_instance_type=&#39;ml.m5.2xlarge&#39;, output_path=output_path, code_location=output_path, ) . When training a custom model on sagemaker you have to define a training directory (source_dir) and a file that serves as the entry point (entry_point). This file must contain the train_fn and the model_fn, predict_fn, input_fn, and output_fn for the inference later on. . The training entry point can be found in sagemaker/src/entry_point.py: . import os import sys # import training function from model import parse_args, train_fn # import deployment functions from model import model_fn, predict_fn, input_fn, output_fn if __name__ == &quot;__main__&quot;: args = parse_args(sys.argv[1:]) train_fn(args) . The training function itself is located in sagemaker/src/model.py. The main difference with a regular model is that we also fit a shap.TreeExplainer to the model and store this in our model directory: . import argparse import joblib import os from pathlib import Path import json import warnings import numpy as np import pandas as pd import shap from sklearn.base import BaseEstimator, TransformerMixin from sklearn.impute import SimpleImputer from sklearn.ensemble import RandomForestClassifier class DFImputer(BaseEstimator, TransformerMixin): def __init__(self, strategy=&quot;median&quot;, fill_value=None): self.imputer = SimpleImputer(strategy=strategy, fill_value=fill_value) self.fitted = False def fit(self, X, y=None): self._feature_names = X.columns self.imputer.fit(X) self.fitted = True return self def transform(self, X): assert self.fitted, &quot;Need to cal .fit(X) function first!&quot; return pd.DataFrame( self.imputer.transform( X[self._feature_names]), columns=self._feature_names, index=X.index ).astype(np.float32) def get_feature_names(self): return self._feature_names class NumpyEncoder(json.JSONEncoder): &quot;&quot;&quot;converts numpy arrays to lists before they get json encoded&quot;&quot;&quot; def default(self, obj): if isinstance(obj, np.ndarray): return obj.tolist() return json.JSONEncoder.default(self, obj) def parse_args(sys_args): parser = argparse.ArgumentParser() parser.add_argument( &quot;--model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;) ) parser.add_argument( &quot;--train-data&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN_DATA&quot;), ) parser.add_argument( &quot;--test-data&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST_DATA&quot;) ) args, _ = parser.parse_known_args(sys_args) return args def train_fn(args): print(&quot;loading data&quot;) train_df = pd.read_csv(args.train_data + &quot;/train.csv&quot;, engine=&#39;python&#39;) test_df = pd.read_csv(args.test_data+ &quot;/test.csv&quot;, engine=&#39;python&#39;) TARGET = &#39;SeriousDlqin2yrs&#39; X_train = train_df.drop(TARGET, axis=1) y_train = train_df[TARGET] X_test = test_df.drop(TARGET, axis=1) y_test = test_df[TARGET] print(&quot;Imputing missing values&quot;) imputer = DFImputer(strategy=&#39;median&#39;).fit(X_train) X_train = imputer.transform(X_train) X_test = imputer.transform(X_test) print(&quot;Building model...&quot;) model = RandomForestClassifier(n_estimators=50, max_depth=6, max_leaf_nodes=30) model.fit(X_train, y_train) explainer = shap.TreeExplainer(model) print(&quot;Saving artifacts...&quot;) model_dir = Path(args.model_dir) model_dir.mkdir(exist_ok=True, parents=True) joblib.dump(imputer, open(str(model_dir / &quot;imputer.joblib&quot;), &quot;wb&quot;)) joblib.dump(model, open(str(model_dir / &quot;model.joblib&quot;), &quot;wb&quot;)) joblib.dump(explainer, open(str(model_dir / &quot;explainer.joblib&quot;), &quot;wb&quot;)) . Inference . For inference we have to define model_fn to load our model artifacts. Here we have to make sure we also load the explainer.joblib. . def model_fn(model_dir): &quot;&quot;&quot;loads artifacts from model_dir and bundle them in a model_assets dict&quot;&quot;&quot; model_dir = Path(model_dir) imputer= joblib.load(model_dir / &quot;imputer.joblib&quot;) model = joblib.load(model_dir / &quot;model.joblib&quot;) explainer = joblib.load(model_dir / &quot;explainer.joblib&quot;) explainer = shap.TreeExplainer(model) model_assets = { &quot;imputer&quot;: imputer, &quot;model&quot;: model, &quot;explainer&quot;: explainer } return model_assets . The function input_fn reads the JSON input and returns a dictionary with a pandas DataFrame. . def input_fn(request_body_str, request_content_type): &quot;&quot;&quot;takes input json and returns a request dict with &#39;data&#39; key&quot;&quot;&quot; assert request_content_type == &quot;application/json&quot;, &quot;content_type must be &#39;application/json&#39;&quot; json_obj = json.loads(request_body_str) if isinstance(json_obj, str): # sometimes you have to unpack the json string twice for some reason. json_obj = json.loads(json_obj) request = { &#39;df&#39;: pd.DataFrame(json_obj) } return request . The predict_fn uses the input dataframe in request and the model assets to calculate the predictions and shap values. . We construct a return dictionary that includes both the prediction, the shap base value (comparable to an intercept in regular OLS), and the shap values per feature: . def predict_fn(request, model_assets): &quot;&quot;&quot; takes a request dict and model_assets dict and returns a response dict with &#39;prediction&#39;, &#39;shap_base&#39; and &#39;shap_values&#39; &quot;&quot;&quot; print(f&quot;data: {request[&#39;df&#39;]}&quot;) features = model_assets[&quot;imputer&quot;].transform(request[&#39;df&#39;]) preds = model_assets[&quot;model&quot;].predict_proba(features)[:, 1] expected_value = model_assets[&quot;explainer&quot;].expected_value if expected_value.shape == (1,): expected_value = expected_value[0].tolist() else: expected_value = expected_value[1].tolist() shap_values = np.transpose(model_assets[&quot;explainer&quot;].shap_values(features)[1]) response = {} response[&#39;prediction&#39;] = preds response[&#39;shap_base&#39;] = expected_value response[&#39;shap_values&#39;] = {k: v for k, v in zip(features.columns.tolist(), shap_values.tolist())} return response . And finally output_fn returns the response in JSON format: . def output_fn(response, response_content_type): &quot;&quot;&quot;takes a response dict and returns a json string of response&quot;&quot;&quot; assert ( response_content_type == &quot;application/json&quot; ), &quot;accept must be &#39;application/json&#39;&quot; response_body_str = json.dumps(response, cls=NumpyEncoder) return response_body_str . Fitting the model . So now we simply fit our model as usual with: . estimator.fit({&#39;train_data&#39;: train_data, &#39;test_data&#39;: test_data}) . deploying the endpoint . And deploy it with: . estimator.deploy( endpoint_name=&quot;credit-explainer&quot;, initial_instance_count=1, instance_type=&#39;ml.c4.xlarge&#39;) . (this can take some time) . Test the endpoint . We build a predictor with appropriate json serializers baked in and test the endpoint: . from sagemaker.predictor import RealTimePredictor from sagemaker.predictor import json_serializer, json_deserializer, CONTENT_TYPE_JSON predictor = RealTimePredictor( endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, deserializer=json_deserializer, content_type=&quot;application/json&quot;, ) predictor.predict(test_df.sample(1).to_json(orient=&#39;records&#39;)) . Output should be something like: . {&#39;prediction&#39;: [0.020315580737022686], &#39;shap_base&#39;: 0.06050333333333335, &#39;shap_values&#39;: {&#39;RevolvingUtilizationOfUnsecuredLines&#39;: [-0.018875482250995595], &#39;age&#39;: [0.0026035737687252584], &#39;NumberOfTime30-59DaysPastDueNotWorse&#39;: [-0.007295913630249845], &#39;DebtRatio&#39;: [-0.001166559449290446], &#39;MonthlyIncome&#39;: [0.00046746497026006246], &#39;NumberOfOpenCreditLinesAndLoans&#39;: [-0.00012379074985687487], &#39;NumberOfTimes90DaysLate&#39;: [-0.010730724822367846], &#39;NumberRealEstateLoansOrLines&#39;: [-0.00029942272129598825], &#39;NumberOfTime60-89DaysPastDueNotWorse&#39;: [-0.004091473195545041], &#39;NumberOfDependents&#39;: [-0.0006754245156943097]}} . lambda + api . Now all that is left to do is set up a lambda function to forward to API call to your endpoint (Sagemaker endpoints can only be reached from within the AWS ecosystem, so you need to put a lambda function in between), and then get a public facing URL from Gateway API. . This tutorial explains the various steps and configurations quite well, so just follow the steps. . In our case we simply forward the event payload onward without any repackaging, so our lambda_handler is quite straightforward. I added a bunch of prints so that we can check our logs in case anything goes wrong. . import os import io import boto3 import json import csv # grab environment variables ENDPOINT_NAME = os.environ[&#39;ENDPOINT_NAME&#39;] runtime= boto3.client(&#39;runtime.sagemaker&#39;) def lambda_handler(event, context): print(&quot;Received event: &quot; + json.dumps(event, indent=2)) response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&#39;application/json&#39;, Body=event) print(&quot;raw response: &quot;, response) result = json.loads(response[&#39;Body&#39;].read().decode()) print(&quot;decoded result: &quot;, result) return result . test api . After setting up the Gateway API you should have a public facing API url so we can test our explainable model endpoint: . # take a single sample row and convert it to JSON: sample_json= df.sample(1)to_json(orient=&#39;records&#39;) # define the header header = {&#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Accept&#39;: &#39;application/json&#39;} # API url, copy your own here: api_url = &quot;https://#########.execute-api.eu-central-1.amazonaws.com/test/credit-explainer&quot; resp = requests.post(api_url, data=json.dumps(sample_json), headers=header) print(resp.json()) . And the result should again be something like this: . {&#39;prediction&#39;: [0.3045473967302875], &#39;shap_base&#39;: 0.06050333333333335, &#39;shap_values&#39;: {&#39;RevolvingUtilizationOfUnsecuredLines&#39;: [0.017002446159576922], &#39;age&#39;: [0.006427313815255611], &#39;NumberOfTime30-59DaysPastDueNotWorse&#39;: [-0.007124554453726655], &#39;DebtRatio&#39;: [-0.006844505153423333], &#39;MonthlyIncome&#39;: [-0.019672520587649577], &#39;NumberOfOpenCreditLinesAndLoans&#39;: [-0.010014011659840212], &#39;NumberOfTimes90DaysLate&#39;: [0.288998818519516], &#39;NumberRealEstateLoansOrLines&#39;: [-0.0007571802810589933], &#39;NumberOfTime60-89DaysPastDueNotWorse&#39;: [-0.022098932417397806], &#39;NumberOfDependents&#39;: [-0.001872810544298378]}} . In this case the customer was predicted to have 30% chance of a loan delinquency in the next two years, mostly based on the number of times that they have been more than 90 days late in their repayments. . Now you can put take this output and embed it in a dashboard where human decision-makers or end customers have access to it! . Good luck building your own explainable machine learning models in AWS Sagemaker! .",
            "url": "https://oegedijk.github.io/blog/markdown/2020/08/14/sagemaker-explainer.html",
            "relUrl": "/markdown/2020/08/14/sagemaker-explainer.html",
            "date": " • Aug 14, 2020"
        }
        
    
  
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://oegedijk.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data scientist working for the UWV (dutch social welfare system), especially interested in machine learning engineering, explainable AI, and getting stuff into production. .",
          "url": "https://oegedijk.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://oegedijk.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}